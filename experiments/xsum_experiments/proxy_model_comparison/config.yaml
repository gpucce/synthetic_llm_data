defs:
  # Here, we define reusable components, each component translate to a number
  # of variables that are instantiated in the template file.
  # e.g. if we define a "datacomp" section, and use it in defining experiments 
  # (see section "experiments below), all the values defined under take their 
  # corresponding value, e.g. if we use datacomp,`train_data` will be replaced by 
  # "/path/{0000000..0139827}.tar" in the sbatch template file.
  # Here, we have only "train_data" for datacomp and laion2b, but we can have a list of 
  # variables. For instance, for s32/m32, we define both `model` and `batch_size`,
  # as the maximum local batch size depend on the model size. Thus, if we 
  # use s32 in defining experiments (see section "experiments below),
  # "model" will take the value "ViT-M-32" and "batch_size" will take the 
  # value "1024" in the sbatch script.

  allmodels:
    genmodel: [
      hf_llama/llama-2-7b-chat-hf,
      hf_mistral/Mistral-7B-Instruct-v0.2,
      hf_camoscio/camoscio1_7b,
      hf_llama/llama-2-13b-chat-hf,
      hf_mistral/Mixtral-8x7B-Instruct-v0.1,
      hf_camoscio/camoscio2_13b_v2,
      hf_llama/llama-13b_change_it,
      hf_llama/llama-2-70b-chat-hf,
      hf_camoscio/camoscio2-70b-lora-hf-decaylr,
    ]

common:
  # Here, we define common variables to all experiments.

  # Path to the sbatch template file, this is the basic squeleton of all sbatch files
  # where variables to be replaced are written as {NAME} (see Step 1)
  template: template.sbatch 
  
  # Path of the standard output file, it is important as it is used for checking:
  # 1 - if the job is frozen (if no change in during `check_interval_secs` secs)
  # 2 - the SLURM job id (`job_id_regexp`), this is important if, for some reason, 
  # the `autoexperiment run <CONFIG>` process is terminated and we want to resume it 
  # while we still have running jobs in SLURM. If it happens, just relaunch 
  # `autoexperiment run <CONFIG>` again, and it will find automatiaclly the SLURM job ids 
  # and continue as before, instead of launching new ones.
  # 3 - to find if the termination string (`termination_str`) appeared in the output file, 
  # this is used to stop from restarting the job forever, and consider it finished.
  # Remember that we have a max time limit in SLURM, 
  # so we restart the job as much as needed until we find the `termination_str`.
  output_file: "{logs}"
  
  # It is IMPORTANT that in the sbatch script (`template.sbatch`), we have a way to 
  # figure out SLURM job id (see explanation above), here we define the regexp used 
  # to find the SLURM job id.
  job_id_regexp: "Job Id:(\\d+)"
  # It is IMPORTANT to define the `termination_str`, it is a regexp used to detect
  # if a job is finished, otherwise, it will be restarted FOREVER.
  # Here, for instance, we detect a finishing job if it finishes the zero-shot 
  # evaluatioof the latest epoch.
  # ({epochs} will take the value of epochs, see section experiments below).
  termination_str: "EVERYTHING DONE"
  
  # an alternative is to use `termination_cmd`, where instead a shell command
  # is executed, if it returns the value 1, the job is considered as finished.
  termination_cmd: "slurm_logs/{set}/{name}/slurm.out"

  # one can also have start condition, where the job is launched only
  # under some constraint. This can be the case for evaluations, for instance,
  # as they require that checkpoints of the models do exist beforehand.
  # Here, we execute the shell command 'start_condition_cmd', if it returns
  # the value 1, the job is launched.
  start_condition_cmd: ""

  # Path of sbatch scripts that are generated from the `template`
  # each experiment will have a dedicated sbatch script.
  sbatch_script: "sbatch/{name}.sbatch"

  # Command to run for each job.
  cmd: "sbatch {sbatch_script}"

  # Check the status jobs each number of secs, to restart them if needed
  check_interval_secs: 600

experiments:
  set7b:
      # variables can either be a list or a single value
      genmodel: [
        hf_llama/llama-7b_xsum,
        hf_mistral/mistral_xsum,
      ]
      lossmodel: [
        hf_llama/llama-2-7b-hf,
        hf_llama/llama-7b_xsum,
        hf_llama/llama-7b_xsum_995_samples,
        hf_mistral/Mistral-7B-v0.1,
        hf_mistral/mistral_xsum,
        hf_mistral/mistral_xsum_995_samples,
      ]
      datapath: /leonardo_scratch/large/userexternal/gpuccett/data/ita_synthetic_data/autoexperiment/xsum/{genmodel}
      outputpath: /leonardo_scratch/large/userexternal/gpuccett/data/ita_synthetic_data/autoexperiment/xsum/comparisons/{genmodel}/{lossmodel}
      logs: "slurm_logs/{set}/{name}/slurm.out"
      nnodes: 1
      taskspernode: 4
      gpuspertask: 1
      cpuspertask: 8
      nsamples: 5000
      maxbatchsize: 16
      name: "ita_data_generate_{genmodel}_{lossmodel}"

  # set13b:

  #     logs: "slurm_logs/{set}/{name}/slurm.out"
  #     genmodel: [
  #       hf_llama/llama-2-13b-hf,
  #       hf_llama/llama-7b_change_it,
  #       hf_llama/llama-13b_change_it,
  #       hf_mistral/mistral_change_it,
  #       hf_llama/llama-2-70b-hf,
  #     ]
  #     lossmodel: [
  #       hf_llama/llama-2-13b-hf,
  #       hf_llama/llama-13b_change_it,
  #       hf_llama/llama-13b_change_it_995_samples,
  #       hf_llama/llama-13b_change_it_3981_samples,
  #       hf_llama/llama-13b_change_it_7962_samples,
  #       hf_camoscio/camoscio2_13b_v2,
  #       hf_mistral/Mixtral-8x7B-Instruct-v0.1,
  #     ]
  #     datapath: /leonardo_scratch/large/userexternal/gpuccett/data/ita_synthetic_data/autoexperiment/xsum/{genmodel}
  #     outputpath: /leonardo_scratch/large/userexternal/gpuccett/data/ita_synthetic_data/autoexperiment/xsum/comparisons/{genmodel}/{lossmodel}
  #     nnodes: 1
  #     taskspernode: 2
  #     gpuspertask: 2
  #     cpuspertask: 16
  #     maxbatchsize: 16
  #     name: "ita_data_generate_{genmodel}_{lossmodel}"
